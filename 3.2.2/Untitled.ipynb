{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97bdfbd-17df-493f-bd03-e9994c129ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"beomi/KoAlpaca-v1.1a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a99137f-5a48-475c-a6e3-0342d30b74b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6de4f1-5dd2-4d0f-911a-ba448374f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dataset is a dictionary with the structure dataset[\"train\"][\"output\"]\n",
    "train_outputs = dataset[\"train\"][\"output\"]\n",
    "\n",
    "# Find the longest sentence using max with a key function\n",
    "longest_sentence = max(train_outputs, key=len)\n",
    "\n",
    "print(\"The longest sentence is:\")\n",
    "print(longest_sentence)\n",
    "print(\"Length of the longest sentence:\", len(longest_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73be6965-3834-4559-be92-b412967fdb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "import torch \n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd05e79f-79b1-4d2c-a07e-031d5c04eb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc719a92025c4ddd8ed496f2f2b5f73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./llama-3-korean-8b-hf/checkpoint-500\",\n",
    "    attn_implementation=\"flash_attention_2\", # SDPA를 사용하고, 대안으로 'flash_attention_2'를 사용할 수 있음\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b4a5f45-ed48-41e4-b583-e8ae33ba4d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ message['content'] }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"{{ '\\n\\nAssistant: ' }}\"\n",
    "    \"{% endif %}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aa448f2-e9b3-4a8c-b1ab-0dfaf38f9bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./llama-3-korean-8b-hf\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aeef57-3cc9-4a9c-b6bd-f974f45c35d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea85b241c04c4172a33aa1350d36892d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.push_to_hub(\"daje/meta-llama3-8B-qna-koalpaca-v1.1\")\n",
    "tokenizer.push_to_hub(\"daje/meta-llama3-8B-qna-koalpaca-v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56769021-4698-410c-b1e4-7f73cf641eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    return_full_text=False,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=2048,\n",
    "    temperature=0.7,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2718d748-dae3-465e-a0c1-b60ea2f92109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "test_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=os.path.join(\"\", \"./test_dataset.json\"),\n",
    "    split=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5a0c0a7-e856-45e4-8634-b95077a44a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ''}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe('케첩과 마요네즈의 입구 모양은 왜 다른가요? \\n보통 케찹과 마요네즈는 한 세트자나요? 그런데 보면 케찹은 원터치 뚜껑에다가 새 케찹은 뚜껑을 돌려서 열면 얇은 알미늄 같은 막도 이꾸또 케찹이 나오는 구멍이 동그랗자나요? 근데 마요네즈는 돌리는 뚜껑에다가 처음에 새로 사도 알미늄 같은 막두 엄꾸나오는 구멍 모양도 별 모양인데 왜 그런가요?'\n",
    "     , max_new_tokens=128\n",
    "     , do_sample=True\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "384a6843-764d-4be7-b2a2-6194963c5d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 2116\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d960b0b3-0793-4ed8-a8a2-86bb0146a50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '당신은 다양한 분야의 전문가들이 제공한 지식과 정보를 바탕으로 만들어진 AI 어시스턴트입니다. 사용자들의 질문에 대해 정확하고 유용한 답변을 제공하는 것이 당신의 주요 목표입니다. 복잡한 주제에 대해서도 이해하기 쉽게 설명할 수 있으며, 필요한 경우 추가 정보나 관련 예시를 제공할 수 있습니다. 항상 객관적이고 중립적인 입장을 유지하면서, 최신 정보를 반영하여 답변해 주세요. 사용자의 질문이 불분명한 경우 추가 설명을 요청하고, 당신이 확실하지 않은 정보에 대해서는 솔직히 모른다고 말해주세요.',\n",
       "  'role': 'system'},\n",
       " {'content': '케첩과 마요네즈의 입구 모양은 왜 다른가요? \\n보통 케찹과 마요네즈는 한 세트자나요? 그런데 보면 케찹은 원터치 뚜껑에다가 새 케찹은 뚜껑을 돌려서 열면 얇은 알미늄 같은 막도 이꾸또 케찹이 나오는 구멍이 동그랗자나요? 근데 마요네즈는 돌리는 뚜껑에다가 처음에 새로 사도 알미늄 같은 막두 엄꾸나오는 구멍 모양도 별 모양인데 왜 그런가요?',\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0][\"messages\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0517e1a-6571-4929-9a53-1bb389f26747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_of_text|>\n",
      "**Query:**\n",
      "해삼을 먹을 때 먹을 수 없는 해삼이 있는가요? 그러한 경우를 어떻게 구분할 수 있을까요?\n",
      "**Original Answer:**\n",
      "  바다에서 스쿠버 다이빙을 하거나 해녀가 수집한 해삼 중 개해삼이라는 종류가 있습니다. 개해삼은 색깔이 고동색으로 전혀 먹을 수 없으며, 목삼이라고도 부릅니다. 목삼은 씹으면 입안이 무거워지며, 이도 찌르는 느낌이 듭니다. 따라서, 개해삼이나 목삼은 먹을 수 없는 해삼입니다. 해삼을 구분하는 방법은 해삼의 색깔과 개체의 모양, 잇빨로 깨물었을 때의 느낌 등으로 가능합니다. \n",
      "\n",
      "해삼은 참해삼(홍해삼, 청해삼), 목삼, 그리고 개해삼으로 나뉩니다. 목삼은 보통 해산물 가공 과정에서 건해삼으로 만들어 식재료로 이용됩니다. 건해삼을 만들기 위해서는 배를 가르고 끓임으로써 물을 제거해야합니다. 김이 올라오면 찬물을 한 번 껴 넣어준 뒤 끓여내면 됩니다. 건조와정에서는 습기가 없는 장소에서 태양과 바람에 말려야합니다. 이 과정은 시간이 걸리며, 건조기를 이용하는 것보다 자연에서 건조할 때 더 좋은 효과를 볼 수 있습니다.\n",
      "\n",
      "참해삼의 경우에는 돌기가 있으며 찌르는 느낌이 들지만, 먹을 수 있는 해삼입니다. 참해삼은 붉은 색의 홍삼과 청록색을 띄는 참해삼이 있으며, 먹는 방법은 다양합니다. 하지만, 개해삼과목삼을 제외하고는 대체로 먹을 수 있는 해삼입니다.\n",
      "**Generated Answer:**\n",
      " 1.5~1.1. \n",
      "- 1.1. 1.5, 1.5, 2.5, 1.5, 1.5, 2.5, 1.5, 2.5. 3.5. 1.5. 2.5. 1.5, 2.5. 3.5. 4.5. 2.5, 2.5. 2.5, 4.5, 2.5, 1.5, 2.5, 2.5, 3.5. 1.5, 3.5, 1.5, 1.5, 4.5, 2.5, 1, 2.5, 3. 2.5, 4. 2. 2. 2. 2. 1. 1. 4. 1. 3. 3. 1. 3. 4. 4. 4. 1. 3. 1. 4. 4. 4. 2. 2. 2. 2. 3. 4. 4. 4. 4. 2. 1. 5. 1. 2. 4. 1. 4. 3. 4. 2. 3. 1. 3. 4. 2. 1. 2. 4. 1. 3. 1. 2. 2. 1. 2. 3. 4. 4. 5. 1. 2. 2. 4. 2. 1. 3. 2. 3. 2. 3. 1. 2. 2. 2. 2. 4. 4. 2. 4. 2. 4. 4. 4. 3. 3. 4. 4. 4. 3. 2. 2. 4. 5. 3. 4. 4. 2. 1. 3. 4. 4. 2. 4. 2. 3. 4. 4. 4. 2. 4. 4. 4. 4. 3. 5. 2. 4. 2. 4. 4. 5. 4. 2. 2. 2. 2. 3. 4. 4. 4. 4. 4. 4. 4. 2. 4. 4. 4. 2. 2. 1. 1. 4. 3. 3. 2. 3. 4. 2. 3. 3. 2. 1. 3. 4. 2. 4. 1. 4. 1. 2. 1. 4. 3. 4. 3. 3. 4. 2. 4. 2. 3. 2. 3. 1. 1. 2. 2. 2. 4. 3. 4. 2. 4. 3. 3. 1. 1. 4. 1. 4. 2. 2. 2. 2. 2. 1. 4. 2. 4. 2. 4. 4. 4. 1. 2. 4. 2. 4. 4. 4. 4.  2. 1. 4. 2. 2. 4. 1. 4. 4. 4. 2. 2. 4. 2.  2. 4. 4.  4. 2004, 4. 2. 3. 1. 4. 2. 3. 4. 4. 4. 2. 2. 4. 2. 4. 2. 2. 4. 3. 1. 4. 4. 2. 4. 2. 4. 4. 1. 4. 4. 1. 3. 3. 4. 3. 4. 2. 4. 4. 4. 4. 4. 2. 4. 4. 4. 2\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randint\n",
    "\n",
    "\n",
    "# Load our test dataset\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "rand_idx = randint(0, len(eval_dataset))\n",
    "messages = eval_dataset[rand_idx][\"messages\"][:2]\n",
    "print(tokenizer.eos_token)\n",
    "# stop generation on eos token or <|eot_id|> token\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "\n",
    "# Test on sample \n",
    "input_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=1024,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(f\"**Query:**\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n",
    "print(f\"**Original Answer:**\\n  {eval_dataset[rand_idx]['messages'][2]['content']}\")\n",
    "print(f\"**Generated Answer:**\\n {tokenizer.decode(response,skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6510cc4-360d-47d3-8f07-fa3ed94af6ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'당신은 다양한 분야의 전문가들이 제공한 지식과 정보를 바탕으로 만들어진 AI 어시스턴트입니다. 사용자들의 질문에 대해 정확하고 유용한 답변을 제공하는 것이 당신의 주요 목표입니다. 복잡한 주제에 대해서도 이해하기 쉽게 설명할 수 있으며, 필요한 경우 추가 정보나 관련 예시를 제공할 수 있습니다. 항상 객관적이고 중립적인 입장을 유지하면서, 최신 정보를 반영하여 답변해 주세요. 사용자의 질문이 불분명한 경우 추가 설명을 요청하고, 당신이 확실하지 않은 정보에 대해서는 솔직히 모른다고 말해주세요.\\n\\nHuman: 2007년에 새롭게 선정된 세계 7대 불가사의가 무엇인지 알고싶습니다. 이전에는 어떤 7대 불가사의였나요?<|end_of_text|>\\n\\nAssistant: 1.1. 1.2, 1.5, 1.2, 2.5, 1.5, 1.1, 1, 2, 2.2, 1, 1, 2, 2.2, 1, 2.5, 1.5, 1.1, 1, 2.2. 1. 1, 2.5, 1. 1, 1, 1, 2.1, 1, 1.2, 1, 1, 1, 1, 1, 1, 1.1, 1, 1, 1, 2. 1, 2, 1, 1, 1, 2.5, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 3, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 3, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fcf32d-8ed1-4bcd-8517-7cb75794765e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
